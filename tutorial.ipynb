{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transition Network Analysis (TNA) Tutorial\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mohsaqr/tnapy/blob/main/tutorial.ipynb)\n\n## Introduction\n\nTransition Network Analysis (TNA) represents a novel methodological approach that captures the temporal and relational dynamics of unfolding processes. The core principle involves representing transition matrices between events as graphs, enabling researchers to leverage graph theory and network analysis comprehensively.\n\nTNA functions as a sophisticated combination of process mining and network analysis. Where process mining typically generates sequential maps, TNA represents these through network analysis \u2014 but with considerably greater analytical depth. The method applies network analysis to capture structure, time, and relationships holistically. Compared to traditional process mining models, TNA incorporates network measures at node, edge, and graph levels, revealing which events hold importance through centrality measures, which transitions prove central, and which processes demonstrate greater connectivity. The method extends beyond standard network analysis by clustering sub-networks into different network constellations representing typical temporal event patterns \u2014 often called tactics.\n\nA distinctive innovation involves statistical validation techniques unavailable in conventional approaches. These include edge verification through bootstrapping, network comparison via permutation testing, and centrality verification through case-dropping methods. These statistical techniques introduce rigor and validation at each analytical step, enabling researchers to verify which edges demonstrate replicability and confirm that inferences remain valid rather than chance artifacts.\n\n### Why TNA?\n\nLearning operates as a complex dynamic system \u2014 a collection of interconnected components interacting across time where interactions can enhance, impede, amplify, or reinforce each other. These dynamic interactions generate emergent behaviors that resist full understanding through analyzing individual components in isolation. Such interactions frequently produce processes exceeding the simple sum of their parts, exhibiting non-linear dynamics.\n\nFor example, motivation catalyzes achievement, which subsequently catalyzes enhanced engagement, enjoyment, and motivation. These interdependencies, feedback loops, and non-linear dynamics create inherent complexity requiring modeling methods transcending traditional linear approaches. TNA, functioning as a dynamic probabilistic model, addresses these limitations by capturing uncertainties through directional probabilities between learning events. The method accommodates the non-linear, evolving character of learning processes while capturing the constellations and emergent patterns defining or shaping learning processes.\n\n### The Building Blocks of TNA\n\nTNA's foundational elements are transitions between events comprising transition processes. A transition represents a conditional relationship between one occurrence and another \u2014 from A to B (a contingency). TNA models transitions in sequential data to compute transition probabilities between events. The resulting transition matrix becomes a weighted directed network where weights represent transition probabilities between events and direction indicates transition direction.\n\n- **Nodes (V)** represent different learning events \u2014 watching videos, taking quizzes, submitting assignments \u2014 or alternatively, states, dialogue moves, collaborative roles, motivation states, or any event representable as sequence units.\n- **Edges (E)** represent transitions between activities, displaying direction from one activity to the next.\n- **Weights (W)** represent transitioning probabilities between events or states.\n\nThis tutorial demonstrates the complete TNA workflow using the Python `tna` package \u2014 from data preparation through model building, visualization, pruning, pattern detection, centrality analysis, community detection, bootstrapping, and group comparison. This tutorial replicates the R TNA tutorial by Saqr & Lopez-Pernas (2025) using the Python implementation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Installation & Setup\n\nTNA can analyze any sequence-representable data with transitions or changes across time \u2014 learning event sequences, states, phases, roles, dialogue moves, or interactions. This data can originate from time-stamped learning management system data, coded interaction data, event-log data, or ordered event data.\n\nInstall the `tna` package and import the required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install tna package (uncomment for Google Colab)\n# !pip install git+https://github.com/mohsaqr/tnapy.git -q\n\nimport tna\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nprint(f\"TNA version: {tna.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Getting Started with Long-Format Data\n\nTNA works with sequential event data. The `tna` package accepts sequence data in several formats: a wide `DataFrame` where rows represent sequences and columns represent timepoints, a transition matrix, or long-format event data that gets reshaped using `prepare_data()`.\n\nThe built-in dataset contains coded collaborative regulation behaviors from learning sessions, with columns for action, actor, and time. Let's start by loading the long-format dataset:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the built-in dataset of coded collaborative regulation behaviors\n",
    "group_regulation_long = tna.load_group_regulation_long()\n",
    "print(f\"Shape: {group_regulation_long.shape}\")\n",
    "group_regulation_long.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a single event with columns:\n",
    "- **Action**: The behavioral state (becomes a network node)\n",
    "- **Actor**: Participant ID (one sequence per actor)\n",
    "- **Time**: Timestamp (for ordering and session splitting)\n",
    "- **Achiever**: Achievement group (High/Low, used later for group comparison)\n",
    "- **Group**: Group identifier\n",
    "- **Course**: Course identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Understanding `prepare_data()`\n\nThe `prepare_data()` function converts long-format event logs into sequences suitable for TNA. It handles session splitting (based on time gaps), ordering, and reshaping. To generate individual sequences for each actor, you must specify both the `actor` and `action` columns.\n\nWhen timestamps are provided via the `time` column, events happening less than 15 minutes apart are grouped in the same sequence, while events occurring after a longer gap mark the start of a new sequence (session). You can customize this gap using the `time_threshold` argument (in minutes).\n\nAn important advantage of using `prepare_data()` prior to constructing the TNA model is that you get to keep other variables of the data (metadata) and use them in your analysis. For instance, you can use `group_tna()` to create a TNA model by achievement group by passing the result of `prepare_data()` and indicating the name of the grouping column."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert long-format event log into sequences for TNA\n",
    "prepared_data = tna.prepare_data(\n",
    "    group_regulation_long,\n",
    "    action=\"Action\",   # column with behavioral states (become network nodes)\n",
    "    actor=\"Actor\",     # column with participant IDs (one sequence per actor)\n",
    "    time=\"Time\"        # column with timestamps (for ordering and session splitting)\n",
    ")\n",
    "prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the wide-format sequence data (rows = sequences, columns = positions)\n",
    "print(\"Sequence data shape:\", prepared_data.sequence_data.shape)\n",
    "prepared_data.sequence_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the preserved metadata (e.g., Achiever group) for each sequence\n",
    "prepared_data.meta_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Alternative Input Formats\n\nIn addition to long-format data processed via `prepare_data()`, TNA models can be built directly from:\n\n- **Wide-format data**: A DataFrame where each row is a sequence and each column represents a time step. This is the most straightforward format when sequences are already aligned.\n- **Pre-computed transition matrices**: A square DataFrame or NumPy array where entry (i, j) represents the transition probability or frequency from state i to state j.\n\nThese alternative inputs provide flexibility for researchers who already have their data in processed formats:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide-format data (rows = sequences, columns = time steps)\n",
    "group_regulation = tna.load_group_regulation()\n",
    "print(\"Wide-format shape:\", group_regulation.shape)\n",
    "group_regulation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-computed transition matrix\n",
    "mat = np.array([\n",
    "    [0.1, 0.6, 0.3],\n",
    "    [0.4, 0.2, 0.4],\n",
    "    [0.3, 0.3, 0.4]\n",
    "])\n",
    "labels = [\"A\", \"B\", \"C\"]\n",
    "model_from_matrix = tna.tna(pd.DataFrame(mat, index=labels, columns=labels))\n",
    "print(model_from_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Importing One-Hot Encoded Data\n\nSome datasets encode states as binary (0/1) indicator columns rather than categorical labels \u2014 for example, coded observation data where each column indicates whether a particular behavior was present in each time interval. The `import_onehot()` function converts this one-hot format into wide-format sequence data suitable for `tna()`.\n\nThe function supports windowing to group multiple time intervals together:\n- **`window_size`**: Number of rows per window (default: 1, each row becomes one time step)\n- **`window_type`**: `'tumbling'` (non-overlapping chunks) or `'sliding'` (step-by-1 overlap)\n- **`aggregate`**: If `True`, collapse each window to the first active state per column (reduces width)\n\nWhen `actor` or `session` columns are provided, windowing is applied within each group, producing one row per actor/session with all windows concatenated.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create example one-hot encoded data (e.g., coded classroom observations)\nonehot_data = pd.DataFrame({\n    \"actor\": [\"s1\"] * 6 + [\"s2\"] * 6,\n    \"Reading\":  [1, 0, 0, 1, 0, 0,  0, 1, 0, 0, 1, 0],\n    \"Writing\":  [0, 1, 0, 0, 1, 0,  1, 0, 0, 1, 0, 0],\n    \"Discuss\":  [0, 0, 1, 0, 0, 1,  0, 0, 1, 0, 0, 1],\n})\nprint(\"One-hot input:\")\nprint(onehot_data)\n\n# Convert to wide-format sequences (one row per actor)\nstates = [\"Reading\", \"Writing\", \"Discuss\"]\nwide_seq = tna.import_onehot(onehot_data, cols=states, actor=\"actor\")\nprint(\"\\nWide-format output:\")\nwide_seq",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Building the TNA Model\n\nTNA analysis begins by building the primary TNA object (called `model`), containing all information necessary for further analysis \u2014 plotting, centrality estimation, or comparison. TNA model estimation employs the `tna()` function, which estimates Markov models from data where initial and transition probabilities derive directly from observed initial state probabilities and transition frequencies.\n\nThe resulting model contains:\n\n- **Initial Probabilities (`inits`)**: Define the likelihood of starting in a particular state at the beginning of the process (the first time point, before transitions). In educational contexts, initial probability represents the probability that students begin in specific states (such as \"engaged\" or \"motivated\") before activities or interventions occur. These probabilities provide a process snapshot showing student starting positions.\n\n- **Transition Probabilities (`weights`)**: Describe state-to-state movement likelihoods at each process step. Transition probabilities capture how students transition, move, or follow between different learning states or events. Each row of the transition matrix sums to 1, representing a complete probability distribution over next states.\n\n- **Labels (`labels`)**: Provide descriptive network node names, enhancing analysis interpretability. Labels automatically derive from the data categories.\n\n- **Data (`data`)**: The sequence data used to build the model, stored internally for further analysis (permutation testing, bootstrapping, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the TNA model from the prepared sequence data\n",
    "model = tna.tna(prepared_data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the transition probability matrix\n",
    "weights_df = model.to_dataframe()\n",
    "weights_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect initial probabilities\n",
    "init_df = pd.Series(model.inits, index=model.labels, name=\"Initial Probability\")\n",
    "init_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Visualizations\n\nTNA model visualization enables bird's-eye views of learning processes, capturing the full structure essence, event connectivity, important pattern identification, and temporal event relationships. TNA provides powerful visualization features with several enhancements for comparing and exploring networks.\n\n### 5.1 Transition Network Plot\n\nThe network plot represents a directed weighted network where each node (state, event, or learning activity) appears as a colored circle. Node-to-node arrows represent weighted transition probabilities with direction showing transition routes. Loops represent identical state repetition probabilities. Edge width and opacity reflect transition probability \u2014 thicker, more opaque edges indicate stronger transitions.\n\nThe `plot_network()` function provides two key parameters for managing visual complexity:\n\n- **`minimum`**: Hides edges below this weight entirely, removing visual clutter. Note that these small probabilities remain in the model for all subsequent computations \u2014 this is purely a visual filter.\n- **`cut`**: Fades edges below this weight (reduced opacity) but still shows them, allowing researchers to see the full network while emphasizing stronger transitions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum: hide edges below 0.05; cut: fade edges below 0.1\n",
    "tna.plot_network(model, minimum=0.05, cut=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Histogram of Edge Weights\n\nExamining the distribution of transition probabilities helps researchers understand the overall structure of the network \u2014 whether transitions are uniformly distributed or concentrated among a few strong connections. This informs decisions about pruning thresholds and helps identify the natural \"backbone\" of the network:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tna.plot_histogram(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Frequency Distribution of States\n\nThe frequency distribution shows how often each state appears as the first event in a sequence, reflecting the initial state probabilities. This helps identify which states learners most commonly begin with and provides context for interpreting the transition network:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of how often each state appears across all sequences\n",
    "tna.plot_frequencies(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Mosaic Plot\n\nThe mosaic (marimekko) plot visualizes the transition matrix as a contingency table. Tile widths are proportional to column totals (incoming transitions) and tile heights are proportional to row proportions (outgoing transitions). Colors represent adjusted standardized residuals from a chi-squared test \u2014 blue tiles indicate more transitions than expected, red tiles indicate fewer. This requires a frequency model built with `ftna()`:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build frequency model and plot mosaic",
    "fmodel = tna.ftna(prepared_data)",
    "tna.plot_mosaic(fmodel)",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Pruning\n\nTransition networks commonly appear fully connected or saturated \u2014 where nearly all nodes connect to all other nodes with some probability. Therefore, mechanisms must retrieve the network core or backbone structure, making networks sparse. Network sparsity enhances interpretability by removing overly complex structures, simplifying important component and relationship identification. It also isolates signal from noise, removing small noisy edges that obscure meaningful patterns, allowing researchers to focus on important interactions.\n\nWhile researchers can use the `minimum` argument in `plot_network()` to visually hide small edges, those small probabilities remain in the model for all subsequent computations. Researchers who want to actually remove negligible-weight edges from the model can use the `prune()` function, which retains only strong, meaningful connections.\n\nThe `prune()` function implements **threshold-based pruning**: edges below a specified threshold value are set to zero (default threshold is 0.05). This provides a clean model where only meaningful transitions remain for downstream analysis.\n\nPruning with TNA can also be accomplished through bootstrapping (demonstrated in the bootstrapping section below), which offers a statistically grounded approach to identifying and eliminating small and uncertain edges."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune: remove edges with weight below 0.05\n",
    "pruned = tna.prune(model, threshold=0.05)\n",
    "\n",
    "print(f\"Original edges: {model.summary()['n_edges']}\")\n",
    "print(f\"Pruned edges:   {pruned.summary()['n_edges']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pruned network\n",
    "tna.plot_network(pruned, cut=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Patterns: Cliques\n\nPatterns help understand behavior, identify significant structures, and describe processes in detail. Patterns form fundamental building blocks of structure and learning process dynamics. They furnish insights into behavior and learner strategies during studying or learning material interaction. Furthermore, capturing repeated consistent patterns enables theory building and generalizable inferences.\n\nTNA supports identifying several n-clique pattern types. Network cliques comprise graph node subsets where every node pair connects directly through edges. In network terms, cliques represent tightly-knit communities, closely related entities, or interdependent nodes shaping learning unfolding.\n\nThe `cliques()` function identifies n-cliques from TNA models. Its arguments include:\n- **`size`**: The clique size to search for (size=2 finds dyads, size=3 finds triads, etc.)\n- **`threshold`**: The minimum edge weight required for an edge to participate in a clique\n\n**Dyads** represent TNA's simplest patterns \u2014 transitions between two nodes. Mutual dyads (bidirectional) with high edge weights indicate strong interdependence through recurrent occurrence. For instance, consistently moving from reading materials to quiz-taking indicates strong self-evaluative strategies.\n\n**Triads** capture more complex three-node relationships. In TNA, three-node cliques where each connects to the others in either direction indicate strong interdependent node subgroups forming a process core. Triads represent higher-order learning behavior dependencies.\n\nWe search for cliques of size 2, 3, and 4 with decreasing thresholds (larger cliques are rarer, so lower thresholds are needed):"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cliques of size 2, 3, and 4 with decreasing thresholds\n",
    "cliques_of_two   = tna.cliques(model, size=2, threshold=0.1)   # dyads\n",
    "cliques_of_three = tna.cliques(model, size=3, threshold=0.05)  # triads\n",
    "cliques_of_four  = tna.cliques(model, size=4, threshold=0.03)  # quads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cliques_of_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cliques_of_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cliques_of_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Centralities\n\nCentrality measures quantify the role or importance of states or events in processes. With centrality measures, researchers can rank events by their value in bridging interactions (betweenness centrality) or receiving the most transitions (in-strength centrality). Centrality measures reveal which behaviors or cognitive states prove central to learning processes \u2014 as frequent transition destinations, starting points for various actions, bridges between learning activities, or keys to spreading phenomena. Using centrality measures, researchers can identify important events to target for intervention or improvement.\n\nImportantly, raw or absolute centrality measure values lack inherent meaning in TNA. Relative values matter instead, allowing node ranking and relative importance identification within networks.\n\n### 8.1 Node-Level Centrality Measures\n\nThe `centralities()` function computes centrality measures using directed probabilistic process algorithms. By default, it removes loops from calculations (changeable via `loops=True`). Removing loops means all centrality computations proceed without considering self-transitioning or same-state repetition.\n\nAvailable measures include:\n- **OutStrength / InStrength**: Sum of outgoing/incoming transition probabilities. In pruned networks where self-loops are removed, out-strength reflects state stability \u2014 higher values indicate greater likelihood of transitioning away.\n- **Closeness / InCloseness**: How quickly a state can reach (or be reached from) all other states.\n- **Betweenness**: How often a state lies on shortest paths between other states, measuring its bridging role.\n- **BetweennessRSP**: Betweenness based on randomized shortest paths \u2014 more appropriate for probabilistic networks.\n- **Diffusion**: Measures how efficiently information or influence spreads from a state.\n- **Clustering**: Local clustering coefficient reflecting the interconnectedness of a state's neighbors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all centrality measures for each state\n",
    "centrality_df = tna.centralities(model)\n",
    "centrality_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot centralities as faceted bar charts\n",
    "tna.plot_centralities(centrality_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.2 Edge-Level Measures: Edge Betweenness\n\nIn TNA, edge centrality measures quantify the importance of transitions between events \u2014 rather than the events themselves \u2014 furnishing insights into particular transitions' criticality for process flow. Edge betweenness centrality reflects how frequently a transition bridges other transitions in the network.\n\nEdge centrality measures help researchers understand not only which nodes are important but which transitions guide learning processes. For instance, a transition from \"planning\" to \"task execution\" might have high edge betweenness, indicating it serves as a critical bridge in the learning process.\n\nThe `betweenness_network()` function creates a new TNA model where edge weights are replaced with their betweenness centrality values:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute edge betweenness for all transitions\n",
    "edge_betweenness = tna.betweenness_network(model)\n",
    "\n",
    "# Show the betweenness values\n",
    "edge_betweenness.to_dataframe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot edge betweenness network\n",
    "tna.plot_network(edge_betweenness, cut=0.1, title=\"Edge Betweenness Network\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Centrality Stability",
    "",
    "Centrality stability assessment determines whether centrality rankings remain consistent when cases are progressively dropped from the data. The `estimate_cs()` function implements the case-dropping bootstrap approach: it repeatedly drops increasing proportions of cases (10% to 90%) and recalculates centralities, measuring rank-order correlation with the original.",
    "",
    "The CS coefficient represents the maximum proportion of cases that can be dropped while maintaining a correlation above 0.7 with at least 95% certainty. CS values above 0.5 indicate stable centrality rankings, while values below 0.25 suggest instability:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Centrality stability: case-dropping bootstrap",
    "cs_result = tna.estimate_cs(model, iter=200, seed=42)",
    "print(\"CS coefficients:\", cs_result.cs_coefficients)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Community Detection\n\nCommunities comprise nodes more closely related or densely interconnected together than to other network nodes. In TNA, communities group states or events that frequently transition between one another or share similar dynamics. Communities represent cohesive sequences or activity successions that are more likely to co-occur, revealing typical pathways or recurring behaviors.\n\nUnlike cliques \u2014 which maintain fixed or predefined structures (2-cliques or 3-cliques) \u2014 communities are data-driven based on connectivity patterns, making them more descriptive of real-world structures. Community identification uncovers latent or hidden clusters of related interaction or behavior during learning. Identifying these clusters provides insight into collaboration and learning effectiveness, common regulatory practices, or interaction patterns.\n\nFurthermore, identifying behavior or event communities can contribute to theory building and learning understanding. These communities represent underlying interaction pattern inferences from densely connected behaviors into simplified meaningful structures, suggesting the presence of underlying constructs or behavioral mechanisms.\n\nThe `communities()` function supports several detection algorithms suited for transition networks (typically small, weighted, and directed):\n\n- **Leading Eigenvector** (`leading_eigen`): Uses the leading eigenvector of the modularity matrix to partition nodes. This is the default method.\n- **Fast Greedy** (`fast_greedy`): Optimizes modularity by iteratively merging communities.\n- **Louvain** (`louvain`): A multi-level modularity optimization algorithm.\n- **Label Propagation** (`label_prop`): Each node adopts the most common community among its neighbors.\n- **Edge Betweenness** (`edge_betweenness`): Iteratively removes high-betweenness edges to reveal communities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect communities using the default algorithm (leading eigenvector)\n",
    "comms = tna.communities(model)\n",
    "print(comms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot communities: nodes colored by community assignment\n",
    "tna.plot_communities(comms, cut=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple community detection methods\n",
    "comms_multi = tna.communities(model, methods=[\"leading_eigen\", \"louvain\", \"fast_greedy\"])\n",
    "print(comms_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Bootstrapping\n\n### 10.1 Why Bootstrap?\n\nBootstrapping represents a robust validation technique for assessing edge-weight accuracy and stability, consequently validating entire models. Through bootstrapping, researchers verify each edge, determine statistical significance, and obtain transition probability confidence intervals. Most network or process mining research employs descriptive methods \u2014 model validation or statistical significance proving remain largely absent from the literature. Validated models enable researchers to assess robustness and reproducibility, ensuring insights arise not from chance and therefore remain generalizable.\n\nBootstrapping \u2014 a resampling technique \u2014 involves repeatedly drawing samples from original datasets **with replacement** to estimate models for each sample (usually hundreds or thousands of times). Bootstrapping requires no strong data distribution assumptions, rendering it suitable for process data analysis that often does not adhere to specific distributions. Given bootstrap replacement, each sample may include multiple copies of some observations while excluding others, assessing parameter estimate variability. Edges consistently appearing across most estimated models prove stable and significant.\n\nAnother key bootstrap advantage involves effectively pruning dense networks. One challenge in probabilistic networks like TNA involves common complete connection \u2014 meaning every possible node connection exists to some degree. Bootstrapping mitigates this by identifying and eliminating small and uncertain edges, effectively retrieving the network backbone. The resulting simplified network proves easier to interpret and more likely to be generalizable.\n\nThe `bootstrap_tna()` function calculates confidence intervals and p-values for each edge weight. The function features a default of 1000 bootstrap iterations (via the `iter` argument). The `level` argument sets the significance threshold (e.g., 0.05) \u2014 if edges consistently appear above this threshold in bootstrapped samples, they are deemed statistically significant."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample sequences 1000 times and assess edge stability\n",
    "np.random.seed(265)  # for reproducibility\n",
    "boot = tna.bootstrap_tna(model, iter=1000, level=0.05, seed=265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 10.2 Results\n\nThe bootstrap result contains several elements:\n\n- **`weights_sig`**: A matrix showing only statistically significant transitions (non-significant weights set to zero)\n- **`weights_mean`**: Mean transition matrix across all bootstrap samples\n- **`weights_sd`**: Standard deviation matrix across all bootstrap samples\n- **`ci_lower` / `ci_upper`**: Bootstrap confidence interval bounds for each transition\n- **`p_values`**: Bootstrap p-value matrix for each transition\n\nThe `summary()` method returns a convenient DataFrame with all of these statistics per edge:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bootstrap summary table\n",
    "boot_df = boot.summary()\n",
    "boot_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only edges that survived the bootstrap and sort by weight\n",
    "sig_edges = boot_df[boot_df[\"sig\"] == True].sort_values(\"weight\", ascending=False)\n",
    "print(f\"{len(sig_edges)} out of {len(boot_df)} edges are significant\")\n",
    "sig_edges.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 10.3 Bootstrapped Network\n\nThe bootstrapped model (`boot.model`) contains only statistically significant edges \u2014 those that survived the bootstrap validation. Plotting this model shows the validated network backbone, which is more likely to generalize to new data:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bootstrapped network (only significant edges)\n",
    "tna.plot_network(boot.model, cut=0.1, title=\"Bootstrapped Network (significant edges)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Sequence Plots\n\nSequence plots provide a direct visualization of the raw sequential data before it is aggregated into a transition network. These visualizations help researchers understand the variety and structure of individual sequences.\n\nTwo plot types are available:\n\n- **Index plot**: Each row represents one sequence, with colors indicating the state at each position. This reveals the diversity and patterns in individual trajectories \u2014 whether sequences are highly varied or follow common templates.\n- **Distribution plot**: Shows the proportion of each state at each sequence position, revealing how the state distribution evolves over time. This helps identify whether certain states dominate at the beginning or end of sequences."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row is one sequence; colors represent states at each position\n",
    "tna.plot_sequences(prepared_data, max_sequences=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of each state at each sequence position\n",
    "tna.plot_sequences(prepared_data, plot_type=\"distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12. Group Models\n\nResearchers frequently encounter predefined conditions \u2014 high versus low achievers, different course types, or gender groups. Comparing such groups has commonly occurred visually \u2014 comparing process models or sequence models. While visual comparison may reveal differences, it fails to indicate statistical significance. Where precisely differences prove statistically significant and where they do not remains unclear.\n\nTNA addresses this by enabling rigorous systematic group comparison. The `group_tna()` function builds separate TNA models for each level of a grouping variable. The metadata preserved by `prepare_data()` (e.g., the **Achiever** column) can be used directly as the grouping variable \u2014 no manual data splitting needed.\n\nAll standard TNA functions (`centralities()`, `prune()`, `communities()`, `cliques()`, `plot_network()`) work seamlessly with group models, automatically applying per-group and returning combined results. This enables researchers to examine how transition dynamics differ across subgroups without writing any group-splitting code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build group models directly from the prepared data using the Achiever metadata column\ngroup_model = tna.group_tna(prepared_data, group=\"Achiever\")\nprint(group_model)\nprint()\n\n# Summary statistics per group\ngroup_model.summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access individual models using dict-style indexing\nprint(group_model[\"High\"])\nprint()\nprint(\"Group names:\", group_model.names())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot all group networks side by side (automatic multi-panel)\ntna.plot_network(group_model, minimum=0.05, cut=0.1)\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "# Prune all groups at once \u2014 returns a new GroupTNA with pruned models\npruned_group = tna.prune(group_model, threshold=0.05)\nprint(pruned_group)\n\n# Compare edge counts\nfor name in group_model:\n    orig = group_model[name].summary()[\"n_edges\"]\n    prun = pruned_group[name].summary()[\"n_edges\"]\n    print(f\"  {name}: {orig} \u2192 {prun} edges\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Centralities across groups \u2014 returns a single DataFrame with a 'group' column\ngroup_cent = tna.centralities(group_model, measures=[\"OutStrength\", \"InStrength\", \"Betweenness\"])\ngroup_cent",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communities per group\n",
    "group_comms = tna.communities(group_model)\n",
    "for name, result in group_comms.items():\n",
    "    print(f\"{name}: {result.counts}\")\n",
    "    print(result.assignments)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.1 Permutation Testing Between Groups\n\nTo address the limitations of simple visual comparison, TNA employs rigorous permutation-based approaches for determining whether observed differences between group models are statistically significant. Permutation tests involve repeatedly shuffling the data between groups and generating a distribution of differences under the null hypothesis. For each edge, the test provides p-values helping researchers identify statistically significant differences. This rigorous approach ensures TNA insights reflect true underlying differences rather than chance artifacts.\n\nThe `permutation_test()` function compares two TNA models by shuffling sequences between groups for a specified number of iterations (`iter`), creating a null distribution of edge-weight differences. Edges where the observed difference exceeds the permutation distribution are flagged as statistically significant.\n\nAccess individual group models with dict-style indexing to compare specific groups:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test: compare High vs Low achievers\n",
    "perm_result = tna.permutation_test(\n",
    "    group_model[\"High\"], group_model[\"Low\"],\n",
    "    iter=500, seed=42, level=0.05\n",
    ")\n",
    "\n",
    "# Show significant edge differences\n",
    "sig_perm = perm_result.edges[\"stats\"][\n",
    "    perm_result.edges[\"stats\"][\"p_value\"] < 0.05\n",
    "].sort_values(\"p_value\")\n",
    "\n",
    "print(f\"{len(sig_perm)} significant edge differences found\")\n",
    "sig_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.2 Difference Network\n\nThe `plot_compare()` function visualizes the difference between two TNA models as a network. Green edges indicate transitions that are stronger in the first model, red edges indicate transitions stronger in the second. Edge width is proportional to the absolute difference. Node colors reflect differences in initial probabilities:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference network: High vs Low achievers\n",
    "tna.plot_compare(group_model[\"High\"], group_model[\"Low\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Sequence Clustering (Tactics)\n\n### 13.1 Why Cluster Sequences?\n\nThe analyses presented so far \u2014 transition networks, centralities, communities, bootstrapping, and group comparison \u2014 all operate at the **network level**, characterizing the aggregate dynamics of how states connect and transition. However, within any dataset, individual sequences often exhibit substantial heterogeneity. Not all learners follow the same behavioral patterns. Some may consistently cycle between planning and execution, while others predominantly engage in monitoring with occasional social interactions.\n\nSequence clustering addresses this heterogeneity by grouping **individual sequences** (learners, sessions, or actors) into clusters of similar behavioral trajectories \u2014 often called **tactics** or **strategies** in educational research. This complements network-level analysis by revealing the diversity of approaches within a population. Where the overall TNA model shows the average transition structure, tactics reveal the distinct behavioral patterns that compose that average.\n\nThis distinction from community detection (Section 9) is important. Communities group **states** that frequently co-transition within the network \u2014 identifying which behaviors tend to co-occur. Tactics group **entire sequences** \u2014 identifying which learners behave similarly across their full trajectory. A learner's tactic reflects their overall strategy, while communities reflect structural relationships among behaviors.\n\nIdentifying tactics serves several research purposes:\n\n- **Typology development**: Discovering naturally occurring behavioral patterns supports theory building about learning strategies, self-regulation approaches, or collaborative styles.\n- **Intervention targeting**: Different tactics may require different interventions. Learners who predominantly monitor but rarely plan may benefit from different support than those who cycle rapidly between all states.\n- **Outcome prediction**: Tactics can predict learning outcomes \u2014 some behavioral patterns may consistently associate with higher or lower achievement.\n- **Group comparison via TNA**: Perhaps most powerfully, discovered tactics can serve as grouping variables for further TNA analysis, building separate transition networks per tactic to examine how transition dynamics differ across behavioral strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2 Distance Metrics\n\nSequence clustering requires measuring how similar or different two sequences are. The `cluster_sequences()` function supports four distance metrics, each capturing different aspects of sequence similarity:\n\n- **Hamming distance** (`'hamming'`): Counts the number of positions where two sequences differ. Fast and intuitive, but requires sequences of equal length and treats all positions equally. Best suited for aligned sequences where positional correspondence matters \u2014 for example, comparing what students did at time step 1, time step 2, etc.\n\n- **Levenshtein distance** (`'lv'`): The minimum number of insertions, deletions, and substitutions needed to transform one sequence into another. Handles unequal-length sequences naturally. Appropriate when the exact temporal alignment is less important than the overall ordering of events.\n\n- **Optimal String Alignment** (`'osa'`): Extends Levenshtein distance by also allowing adjacent transpositions (swapping two neighboring elements). Useful when near-swaps should be considered minor differences \u2014 for example, if planning-then-executing versus executing-then-planning represents a small rather than large behavioral difference.\n\n- **Longest Common Subsequence** (`'lcs'`): Measures distance as the length difference after removing the longest shared subsequence. Focuses on what two sequences have in common regardless of position. Particularly appropriate when the presence of certain behavioral patterns matters more than their exact timing.\n\nFor most TNA applications with aligned sequences from `prepare_data()`, Hamming distance provides a good default. For sequences of varying length or when positional alignment is uncertain, Levenshtein or LCS distances are more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sequences into 3 tactics using PAM with Hamming distance\nclust = tna.cluster_sequences(prepared_data, k=3, dissimilarity=\"hamming\", method=\"pam\")\nprint(clust)\nprint(f\"\\nCluster sizes: {clust.sizes}\")\nprint(f\"Silhouette score: {clust.silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3 Clustering Methods\n\nTwo families of clustering methods are available:\n\n**Partitioning Around Medoids (PAM)** (`method='pam'`) is the default and generally recommended method. PAM identifies *medoid* sequences \u2014 actual sequences from the data that best represent each cluster center. Unlike k-means (which uses abstract centroids), PAM's medoids are real, interpretable sequences that researchers can examine directly. PAM is also more robust to outliers than centroid-based methods.\n\n**Hierarchical clustering** builds a tree-like dendrogram by iteratively merging the most similar sequences or clusters. The tree is then cut at the desired number of clusters. Several linkage methods control how inter-cluster distances are computed:\n\n- `'complete'` \u2014 maximum distance between any pair across clusters (produces compact, spherical clusters)\n- `'average'` \u2014 mean distance between all pairs across clusters (balanced approach)\n- `'ward.D'` / `'ward.D2'` \u2014 minimizes within-cluster variance (tends to produce equal-sized clusters)\n- `'single'` \u2014 minimum distance between any pair across clusters (can produce elongated, chain-like clusters)\n\nThe choice of method and distance metric can substantially affect results. Comparing multiple configurations and evaluating silhouette scores helps identify the most meaningful clustering for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distance metrics (using first 200 sequences for slower metrics)\nsubset = prepared_data.sequence_data.iloc[:200]\nprint(\"Distance metric comparison (PAM, k=3, n=200):\")\nfor metric in [\"hamming\", \"lcs\", \"osa\"]:\n    result = tna.cluster_sequences(subset, k=3, dissimilarity=metric)\n    print(f\"  {metric:>7s}: sizes={result.sizes}, silhouette={result.silhouette:.4f}\")\n\nprint(\"\\nLinkage method comparison (Hamming, k=3, full data):\")\nfor method in [\"pam\", \"complete\", \"average\", \"ward.D2\"]:\n    result = tna.cluster_sequences(prepared_data, k=3, method=method)\n    print(f\"  {method:>8s}: sizes={result.sizes}, silhouette={result.silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.4 Choosing the Number of Clusters\n\nSelecting the appropriate number of clusters is a critical decision. The **silhouette score** measures how well each sequence fits its assigned cluster compared to the nearest alternative cluster. Values range from -1 (poor fit) to +1 (excellent fit), with higher mean silhouette scores indicating better-separated, more cohesive clusters.\n\nA practical approach is to compute silhouette scores for a range of *k* values and select the *k* that maximizes the score \u2014 or the *k* where the score plateaus, indicating diminishing returns from additional clusters. Domain knowledge should also inform the decision: the identified clusters should be interpretable and meaningful in the research context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep k values and compare silhouette scores\nprint(\"Silhouette scores for different k values:\")\nfor k in range(2, 6):\n    result = tna.cluster_sequences(prepared_data, k=k)\n    print(f\"  k={k}: silhouette={result.silhouette:.4f}, sizes={result.sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5 Building TNA Models per Tactic\n\nThe most powerful application of sequence clustering in TNA is using the discovered tactics as grouping variables to build separate transition networks per cluster. This reveals how transition dynamics differ across behavioral strategies \u2014 for example, whether learners in a \"monitoring-heavy\" tactic show different transition patterns than those in a \"planning-heavy\" tactic.\n\nThe workflow is straightforward: cluster the sequences, assign each sequence to its tactic, and use `group_tna()` with the tactic labels as the grouping variable. All standard TNA functions \u2014 centralities, pruning, communities, bootstrapping, permutation testing \u2014 then work seamlessly on the tactic-based group model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Cluster sequences into tactics\nclust = tna.cluster_sequences(prepared_data, k=3, dissimilarity=\"hamming\", method=\"pam\")\n\n# Step 2: Add tactic labels to the sequence data\ntactic_data = prepared_data.sequence_data.copy()\ntactic_data[\"Tactic\"] = [f\"Tactic {c}\" for c in clust.assignments]\n\n# Step 3: Build a TNA model for each tactic\ntactic_model = tna.group_tna(tactic_data, group=\"Tactic\")\nprint(tactic_model)\nprint()\ntactic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transition networks across tactics\ntna.plot_network(tactic_model, minimum=0.05, cut=0.1)\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralities per tactic \u2014 which states are central in each behavioral strategy?\ntactic_cent = tna.centralities(tactic_model, measures=[\"OutStrength\", \"InStrength\", \"Betweenness\"])\ntactic_cent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Complete Workflow at a Glance\n\nThe following code summarizes the full TNA analysis pipeline. This can serve as a template for your own analyses:\n\n```python\nimport tna\nimport pandas as pd\n\n# 1. Load and prepare data\nmy_data = pd.read_csv(\"your_data.csv\")\nprepared = tna.prepare_data(my_data, action=\"event\", actor=\"user_id\", time=\"timestamp\")\n\n# 2. Build model\nmodel = tna.tna(prepared)\n\n# 3. Visualize\ntna.plot_network(model, minimum=0.05, cut=0.1)\ntna.plot_histogram(model)\ntna.plot_frequencies(model)\n\n# 4. Prune\npruned = tna.prune(model, threshold=0.05)\ntna.plot_network(pruned, cut=0.1)\n\n# 5. Cliques\nprint(tna.cliques(model, size=2, threshold=0.1))\nprint(tna.cliques(model, size=3, threshold=0.05))\n\n# 6. Centralities\ntna.plot_centralities(tna.centralities(model))\ntna.plot_network(tna.betweenness_network(model), cut=0.1)\n\n# 7. Communities\ntna.plot_communities(tna.communities(model), cut=0.1)\n\n# 8. Bootstrap\nboot = tna.bootstrap_tna(model, iter=1000, level=0.05, seed=265)\ntna.plot_network(boot.model, cut=0.1)\n\n# 9. Sequences\ntna.plot_sequences(prepared)\n\n# 10. Group models (from metadata column)\ngm = tna.group_tna(prepared, group=\"achievement\")\ntna.plot_network(gm)                    # side-by-side networks\ntna.centralities(gm)                    # centralities with group column\ntna.prune(gm, threshold=0.05)           # prune each group\ntna.permutation_test(gm[\"A\"], gm[\"B\"])  # compare two groups\n\n# 11. Sequence clustering (tactics)\nclust = tna.cluster_sequences(prepared, k=3, dissimilarity=\"hamming\")\ntactic_data = prepared.sequence_data.copy()\ntactic_data[\"Tactic\"] = [f\"Tactic {c}\" for c in clust.assignments]\ntactic_model = tna.group_tna(tactic_data, group=\"Tactic\")\ntna.plot_network(tactic_model)          # per-tactic networks\n\n# 12. Import one-hot encoded data\nwide = tna.import_onehot(onehot_df, cols=[\"State_A\", \"State_B\"], actor=\"id\")\nmodel_oh = tna.tna(wide)\n```\n\nFor more information, see the [TNA package documentation](https://github.com/mohsaqr/tnapy) and the [R TNA tutorial](https://lamethods.org/book2/chapters/ch15-tna/ch15-tna.html) by Saqr & Lopez-Pernas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}