# Project Learnings

### 2026-02-15
- [closeness centrality]: igraph::closeness() defaults to normalized=FALSE (returns 1/sum(distances)), not normalized=TRUE ((n-1)/sum(distances)). Python implementation was using n_reachable/total_dist which equals (n-1)/total_dist for fully connected graphs — 8x off for the 9-state group_regulation dataset.
- [closeness mode=all]: igraph mode="all" treats directed edges as bidirectional. For each edge pair (u,v), the effective undirected weight is max(w(u,v), w(v,u)). NetworkX's DiGraph.to_undirected() does NOT properly combine bidirectional edge weights — must manually build undirected graph with max weight per edge pair.
- [ctna implementation]: Python ctna counts only adjacent bidirectional co-occurrences (each adjacent pair counted in both directions). R ctna uses window-based co-occurrence counting, producing much larger counts. Raw co-occurrence matrices are intentionally different between R and Python.
- [R TNA centralities]: R TNA 1.2.0 calls igraph functions with weights=1/edge_weight (invert=TRUE default). The centrality_funs dispatch closeness with mode="in"/"out"/"all" and betweenness without normalized parameter.
- [test validation]: Original test_r_equivalence.py only checked structural properties (shapes, ranges, non-negative, finite). Never compared against actual R output values. Tests can pass while being numerically wrong.
- [R ground truth]: Full precision R values obtained from R TNA 1.2.0 / igraph 2.2.1. Weight matrix and centralities match Python within ~1e-15 (machine epsilon) after fixes.
- [R bootstrap algorithm]: R TNA bootstrap resamples **per-sequence 3D transition arrays** (not raw sequences). Core unit: `trans[n_sequences, n_states, n_states]` built via `compute_transitions()`, then `trans[sample(idx, n, replace=TRUE), , ]`. Original Python resampled raw rows — fundamentally different algorithm.
- [R bootstrap stability]: R stability method counts exceedances `(wb <= w*0.75) + (wb >= w*1.25)` per iteration. P-values = `(count+1)/(iter+1)` — can exceed 1.0 for zero-weight edges (both conditions fire). This is intentional R behavior.
- [R permutation test]: R permutation combines 3D transitions from both groups, shuffles sequence indices, splits into two groups, computes weight difference per edge. Effect size = `diff_true / sd(perm_diffs)` with `ddof=1`.
- [R as.vector column-major]: R's `as.vector()` outputs matrices in column-major order (column 1 first, then column 2...). When comparing 81-element vectors from R 9x9 matrices, must use `order='F'` (Fortran/column-major) when reshaping in Python.
- [R p.adjust]: R's `p.adjust()` Holm method sorts ascending, applies `max(1, n-i+1) * p[i]`, enforces monotonicity with cumulative max, then restores original order. BH/FDR sorts ascending, applies `n/rank * p`, enforces monotonicity with cumulative min from the right.

### 2026-02-16
- [matplotlib arrows]: FancyArrowPatch arrowstyle `->,head_length=X,head_width=Y` combined with `mutation_scale` controls arrowhead size. Original `/15` and `/25` ratios with `mutation_scale=15` produce small arrows. Ratios of `/10` and `/15` with `mutation_scale=20` were too large. Final sweet spot: `/15` and `/22` with `mutation_scale=15`.
- [edge label placement]: Positioning edge labels at 65% along the edge towards the destination (`x1 + 0.65 * (x2 - x1)`) makes labels visually associate with their target node, much clearer than the default midpoint (50%).
- [notebook DPI]: Default `figure.dpi = 100` produces low-resolution plots in HTML exports. `150` is a good balance for quality vs file size.
- [jupyter kernel]: The project has a dedicated `tnapy` kernel (at `~/Library/Jupyter/kernels/tnapy/`) using the `.venv` virtualenv. The system Python (`/opt/homebrew/bin/python3`) lacks numpy/pandas. Always use `--ExecutePreprocessor.kernel_name=tnapy` when running `nbconvert --execute`.
- [GroupTNA duck typing]: Used `_is_group_tna(x)` with `hasattr(x, 'models')` to avoid circular imports between `group.py` and other modules (`centralities.py`, `prune.py`, `communities.py`, `cliques.py`, `bootstrap.py`, `plot.py`).
- [GroupTNA dispatch pattern]: Each analysis function checks `_is_group_tna(model)` at the top and dispatches per-group, returning combined results (DataFrame with 'group' column for centralities, GroupTNA for prune, dict for communities/cliques/bootstrap).
- [plot GroupTNA]: Multi-panel plots use `fig_w = figsize[0] * n_groups` to scale figure width by group count, with `plt.subplots(1, n_groups)`. Must handle `n_groups == 1` case where `axes` is not a list.
- [estimate_cs algorithm]: Case-dropping bootstrap uses `rng.choice(n, size=n_keep, replace=False)` — WITHOUT replacement, key difference from regular bootstrap. CS coefficient = max drop_prop where >=95% of correlations stay above 0.7. Zero-SD measures (constant across states) get CS=0.
- [installed vs local package]: When running scripts from `tmp/`, Python may pick up a stale *installed* copy from `.venv/lib/.../site-packages/tna/` instead of the local `tna/` source. Running `pip install -e .` ensures editable mode resolves this.
- [chi2_contingency]: `scipy.stats.chi2_contingency` requires at least 2 non-zero rows and 2 non-zero columns. Must filter out zero-sum rows/cols before calling. Standardized residuals = `(observed - expected) / sqrt(expected)`.
- [plot_compare]: Difference network colors edges by sign of `x.weights - y.weights` (green=positive, red=negative). Node fill alpha scaled by `|init_diff|`. Must reorder y's weights/inits to match x's label order before computing diff.
- [R calculate_cs]: R uses `max(which(prop_above >= certainty))` — finds ALL valid drop proportions and takes the maximum. NOT sequential break (correlations can dip below threshold then recover). Python `_calculate_cs` must use `np.where()` + `max`, not loop-with-break.
- [R n_drop]: R uses `n_drop <- floor(n * prop)` and `keep <- sample(n_seq, n - n_drop)`. Python must use `int(np.floor(n * dp))` not `int(round(n * (1 - dp)))` — difference of 1 for some n/dp combinations.
- [R plot_mosaic transpose]: R's `plot_mosaic` uses `as.table(t(weights))` — transposed weight matrix. X-axis = "Incoming edges" (to-states), Y-axis = "Outgoing edges" (from-states). Only supported for frequency/co-occurrence types.
- [R chisq.test stdres]: R's `chisq.test()$stdres` are **adjusted** standardized residuals: `(O - E) / sqrt(E * (1 - ri/N) * (1 - cj/N))`, NOT simple Pearson residuals `(O - E) / sqrt(E)`. The denominator accounts for marginal proportions. R uses fixed color limits [-4, 4] for the mosaic colorbar.
- [R TNA LCS formula]: R TNA `lcs_dist` uses `max(m, n) - lcs_length`, NOT the standard `m + n - 2 * lcs_length`. These give different results for unequal-length sequences. Python implementation uses R's formula for compatibility.
- [R TNA lv/osa bug]: R TNA 1.2.0 `levenshtein_dist` and `osa_dist` have inverted cost: `0L + 1L * (x[i] == y[j])` assigns cost=1 for matches and cost=0 for mismatches, producing incorrect distances (most pairs distance 0). Python implementation uses the correct standard algorithm.
- [scipy fcluster ties]: `scipy.cluster.hierarchy.fcluster(Z, t=k, criterion='maxclust')` fails when the last merge heights are tied — may return fewer clusters than requested. Use `cut_tree(Z, n_clusters=k)` instead, which matches R's `cutree()` behavior.
- [R import_onehot format]: R's `import_onehot` produces one row per actor/group with W{w}_T{t} column naming (all windows concatenated). Python version produces one row per window with action_{i} columns. Different output shapes but both compatible with TNA analysis.
- [cluster_sequences R match]: Hamming distances, LCS distances, PAM clustering (assignments, silhouette, sizes), and hierarchical clustering (complete, average) all match R TNA exactly. Minor differences possible in hierarchical methods when merge distances are tied (tie-breaking differs between scipy and R hclust).
- [compare_sequences R match]: Frequencies (918 patterns) and proportions match R exactly to machine epsilon (~1e-16). R computes proportions per subsequence-length group BEFORE min_freq filtering (denominator includes all patterns, not just filtered ones). R runs permutation test on ALL patterns before filtering, so Bonferroni uses full pattern count per length. Output sorted by length then alphabetical (no test) or by p_value (with test).
- [compare_sequences permutation]: Pre-computing (row_index → pattern_indices) mapping speeds permutation loop ~3x: only need to re-accumulate counts by permuted group, not re-extract patterns each iteration.
- [compare_sequences p_adjust grouping]: R adjusts p-values separately per subsequence length group. E.g. 9 unigrams get Bonferroni factor 9, 78 bigrams get factor 78. Each raw p = 1/(iter+1) when all permuted stats < true stat.
